{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca639cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.utils import shuffle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f510c031",
   "metadata": {},
   "source": [
    "# load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a1da342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../FINAL_GEMINI_CHATGPT_FULL_fixed_20250602_230757.csv')\n",
    "print(len(df))\n",
    "\n",
    "# # only when you want to use advers as validation\n",
    "# df = pd.read_csv('../FINAL_GEMINI_CHATGPT_DEFAULT_fixed_20250602_230832.csv')\n",
    "# advers_df = pd.read_csv('../FINAL_GEMINI_CHATGPT_ADVERS_fixed_20250602_230658.csv')\n",
    "# print(len(advers_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddeaa45",
   "metadata": {},
   "source": [
    "#### --- 1. Data Preparation ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3fe747c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique documents: 3257\n",
      "Test documents: 651\n",
      "Test set size: 10039\n",
      "Training set size: 39961\n"
     ]
    }
   ],
   "source": [
    "def split_dataframe_by_document_ids(df, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Split dataframe into train and validation sets ensuring documents stay together.\n",
    "    \"\"\"\n",
    "    # Get unique document IDs and calculate split\n",
    "    unique_ids = set(df['document_id'])\n",
    "    num_test_docs = int(len(unique_ids) * test_size)\n",
    "    \n",
    "    # Randomly select test documents\n",
    "    test_ids = np.random.choice(list(unique_ids), size=num_test_docs, replace=False)\n",
    "    \n",
    "    # Split dataframe\n",
    "    val_df = df[df['document_id'].isin(test_ids)].copy()\n",
    "    train_df = df[~df['document_id'].isin(test_ids)].copy()\n",
    "    \n",
    "    print(f\"Unique documents: {len(unique_ids)}\")\n",
    "    print(f\"Test documents: {len(test_ids)}\")\n",
    "    print(f\"Test set size: {len(val_df)}\")\n",
    "    print(f\"Training set size: {len(train_df)}\")\n",
    "    \n",
    "    return train_df, val_df\n",
    "\n",
    "def split_dataframe_by_model(df, test_size=0.2, model_name='chatgpt'):\n",
    "    # Get unique document IDs and calculate split\n",
    "    unique_ids = set(df['document_id'])\n",
    "    num_test_docs = int(len(unique_ids) * test_size)\n",
    "    \n",
    "    # Randomly select test documents\n",
    "    test_ids = np.random.choice(list(unique_ids), size=num_test_docs, replace=False)\n",
    "    \n",
    "    # Split dataframe\n",
    "    val_df = df[df['document_id'].isin(test_ids)].copy()\n",
    "    val_df = val_df[val_df['source'] != model_name]\n",
    "    train_df = df[~df['document_id'].isin(test_ids)].copy()\n",
    "    train_df = train_df[train_df['source'] == model_name]\n",
    "    print(f\"Unique documents: {len(unique_ids)}\")\n",
    "    print(f\"Test documents: {len(test_ids)}\")\n",
    "    print(f\"Test set size: {len(val_df)}\")\n",
    "    print(f\"Training set size: {len(train_df)}\")\n",
    "    return train_df, val_df\n",
    "\n",
    "def split_dataframe_by_prompt(df, test_size=0.2, task='rephrase'):\n",
    "    # Get unique document IDs and calculate split\n",
    "    unique_ids = set(df['document_id'])\n",
    "    num_test_docs = int(len(unique_ids) * test_size)\n",
    "    \n",
    "    # Randomly select test documents\n",
    "    test_ids = np.random.choice(list(unique_ids), size=num_test_docs, replace=False)\n",
    "    \n",
    "    # Split dataframe\n",
    "    val_df = df[df['document_id'].isin(test_ids)].copy()\n",
    "    val_df = val_df[val_df['task'] == task]\n",
    "    train_df = df[~df['document_id'].isin(test_ids)].copy()\n",
    "    train_df = train_df[train_df['task'] != task]\n",
    "    print(f\"Unique documents: {len(unique_ids)}\")\n",
    "    print(f\"Test documents: {len(test_ids)}\")\n",
    "    print(f\"Test set size: {len(val_df)}\")\n",
    "    print(f\"Training set size: {len(train_df)}\")\n",
    "    return train_df, val_df\n",
    "\n",
    "# Split the data by document ids / model / prompt\n",
    "train_df, val_df = split_dataframe_by_document_ids(df)\n",
    "# Or use your own split\n",
    "# train_df =df\n",
    "# val_df = advers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "139b7895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists of texts and corresponding labels\n",
    "train_texts = train_df['chapter'].tolist() + train_df['generated'].tolist()\n",
    "train_labels = [0] * len(train_df) + [1] * len(train_df)\n",
    "\n",
    "val_texts = val_df['chapter'].tolist() + val_df['generated'].tolist()\n",
    "val_labels = [0] * len(val_df) + [1] * len(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea4516d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_texts(texts, max_length=4*128):\n",
    "    return [text[:max_length] for text in texts]\n",
    "\n",
    "# Apply truncation to both training and validation texts\n",
    "train_texts = truncate_texts(train_texts)\n",
    "val_texts = truncate_texts(val_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d4247d",
   "metadata": {},
   "source": [
    "#### --- 2. Load Tokenizer and Model ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e80b424b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"google-bert/bert-base-multilingual-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43bfcd6",
   "metadata": {},
   "source": [
    "#### --- 3. Tokenize Data ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9561adcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the texts\n",
    "train_encodings = tokenizer(train_texts, padding=\"max_length\", truncation=True)\n",
    "val_encodings = tokenizer(val_texts, padding=\"max_length\", truncation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5cf3003",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Ensure all encoding keys are converted to tensors\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        # Use the length of one of the encoding lists (e.g., 'input_ids')\n",
    "        return len(self.encodings['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d189a40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextDataset(train_encodings, train_labels)\n",
    "val_dataset = TextDataset(val_encodings, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6af900",
   "metadata": {},
   "source": [
    "#### --- 5. Set up Trainer ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5c958fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results_multilingual_bert',          # Output directory for checkpoints and logs\n",
    "    num_train_epochs=1,              # Reduce epochs for a quicker example run\n",
    "    per_device_train_batch_size=8,   # Adjust based on GPU memory\n",
    "    per_device_eval_batch_size=16,  # Adjust based on GPU memory\n",
    "    warmup_steps=100,                # Number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # Strength of weight decay\n",
    "    logging_dir='./logs',            # Directory for storing logs\n",
    "    logging_steps=50,               # Log metrics every 50 steps\n",
    "    eval_strategy=\"steps\",           # Evaluate during training\n",
    "    eval_steps=500,                  # Evaluate every 500 steps\n",
    "    save_strategy=\"steps\",           # Save checkpoint strategy\n",
    "    save_total_limit=2,             # Limit the total amount of checkpoints\n",
    "    save_steps=1000,                  # Save checkpoint every 500 steps\n",
    "    load_best_model_at_end=True,     # Load the best model found during training\n",
    "    metric_for_best_model=\"accuracy\", # Use accuracy to determine the best model\n",
    "    greater_is_better=True,\n",
    "    fp16=torch.cuda.is_available(),  # Use mixed precision if CUDA is available\n",
    "    report_to=\"none\"                   # Disable reporting to wandb/tensorboard for this example\n",
    ")\n",
    "\n",
    "# Define evaluation metric (accuracy)\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds)\n",
    "    false_positive_rate = np.sum((preds == 1) & (labels == 0)) / np.sum(labels == 0)\n",
    "    false_negative_rate = np.sum((preds == 0) & (labels == 1)) / np.sum(labels == 1)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'false_positive_rate': false_positive_rate,\n",
    "        'false_negative_rate': false_negative_rate\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e1d499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6001' max='9970' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6001/9970 19:49 < 13:07, 5.04 it/s, Epoch 0.60/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>False Positive Rate</th>\n",
       "      <th>False Negative Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.363400</td>\n",
       "      <td>0.370366</td>\n",
       "      <td>0.866048</td>\n",
       "      <td>0.874363</td>\n",
       "      <td>0.200138</td>\n",
       "      <td>0.067766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.311900</td>\n",
       "      <td>0.351191</td>\n",
       "      <td>0.892324</td>\n",
       "      <td>0.893315</td>\n",
       "      <td>0.116961</td>\n",
       "      <td>0.098390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.258200</td>\n",
       "      <td>0.353431</td>\n",
       "      <td>0.883829</td>\n",
       "      <td>0.891643</td>\n",
       "      <td>0.188284</td>\n",
       "      <td>0.044058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.299400</td>\n",
       "      <td>0.261897</td>\n",
       "      <td>0.921762</td>\n",
       "      <td>0.920210</td>\n",
       "      <td>0.058777</td>\n",
       "      <td>0.097698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.278400</td>\n",
       "      <td>0.262783</td>\n",
       "      <td>0.926702</td>\n",
       "      <td>0.925659</td>\n",
       "      <td>0.059271</td>\n",
       "      <td>0.087326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.349500</td>\n",
       "      <td>0.271461</td>\n",
       "      <td>0.911192</td>\n",
       "      <td>0.904797</td>\n",
       "      <td>0.021634</td>\n",
       "      <td>0.155981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.259100</td>\n",
       "      <td>0.234867</td>\n",
       "      <td>0.922355</td>\n",
       "      <td>0.923302</td>\n",
       "      <td>0.089993</td>\n",
       "      <td>0.065297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.248500</td>\n",
       "      <td>0.219786</td>\n",
       "      <td>0.933765</td>\n",
       "      <td>0.932549</td>\n",
       "      <td>0.048207</td>\n",
       "      <td>0.084264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.249600</td>\n",
       "      <td>0.221509</td>\n",
       "      <td>0.936531</td>\n",
       "      <td>0.934234</td>\n",
       "      <td>0.028549</td>\n",
       "      <td>0.098390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.244300</td>\n",
       "      <td>0.269349</td>\n",
       "      <td>0.931740</td>\n",
       "      <td>0.931652</td>\n",
       "      <td>0.066976</td>\n",
       "      <td>0.069545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.242400</td>\n",
       "      <td>0.250208</td>\n",
       "      <td>0.926800</td>\n",
       "      <td>0.928812</td>\n",
       "      <td>0.101452</td>\n",
       "      <td>0.044947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.211200</td>\n",
       "      <td>0.246739</td>\n",
       "      <td>0.933765</td>\n",
       "      <td>0.934944</td>\n",
       "      <td>0.084362</td>\n",
       "      <td>0.048108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize Trainer and Train\n",
    "trainer = Trainer(\n",
    "    model=model,                         # The instantiated Transformers model to be trained\n",
    "    args=training_args,                  # Training arguments, defined above\n",
    "    train_dataset=train_dataset,         # Training dataset\n",
    "    eval_dataset=val_dataset,            # Evaluation dataset\n",
    "    compute_metrics=compute_metrics      # Function to compute metrics\n",
    ")\n",
    "\n",
    "# --- 6. Train the Model ---\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# --- 7. Evaluate the Model ---\n",
    "print(\"Evaluating model...\")\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Evaluation results: {eval_results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b33f7c",
   "metadata": {},
   "source": [
    "#### --- Optional: Save the fine-tuned model and tokenizer ---Please check name--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3669dcbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving/Updating model and tokenizer to ./fine_tuned_romanian_bert_128_full2...\n",
      "Model and tokenizer saved to ./fine_tuned_romanian_bert_128_full2\n"
     ]
    }
   ],
   "source": [
    "save_path = \"./fine_tuned_multilingual_bert\"\n",
    "\n",
    "# Save model and tokenizer, overwriting if it already exists\n",
    "print(f\"Saving/Updating model and tokenizer to {save_path}...\")\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "print(f\"Model and tokenizer saved to {save_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
