{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import bitsandbytes as bnb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils_gen import Rephraser, to_csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load the API key from .env file\n",
    "load_dotenv()\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "# google_model_name = \"gemini-2.0-flash\"\n",
    "# google_model_name = \"gemma-3-27b-it\"\n",
    "# google_model_name = \"gemini-2.5-flash-preview-04-17\"\n",
    "google_model_name = \"gemini-2.5-flash-preview-05-20\"\n",
    "\n",
    "# openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "# openai_model_name = \"gpt-4o-mini\"\n",
    "\n",
    " \n",
    "# hf_api_key = os.getenv(\"HUGGINGFACE_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "class GoogleAIClient(Rephraser):\n",
    "    def __init__(self, api_key, model_name):\n",
    "        super().__init__()\n",
    "        self.client = genai.Client(api_key=api_key)\n",
    "        # self.client = genai.Client()\n",
    "        self.model_name = model_name\n",
    "            \n",
    "    \n",
    "    def gen(self, text_to_rephrase, query_task, temperature):\n",
    "        query = self.compute_query(text_to_rephrase, query_task)\n",
    "        # print(f\"Query: {query}\")\n",
    "        # query = compute_costum_query(text_to_rephrase, query_task)\n",
    "        \n",
    "        response = self.client.models.generate_content(\n",
    "            model= self.model_name,\n",
    "            contents=query,\n",
    "            config=types.GenerateContentConfig(\n",
    "                thinking_config = types.ThinkingConfig(\n",
    "                    thinking_budget=0,\n",
    "                ),\n",
    "                temperature= temperature,\n",
    "                max_output_tokens= 4096 ,\n",
    "                candidate_count=1,\n",
    "                response_mime_type=\"text/plain\",\n",
    "                system_instruction=[ # NOT AVAIALBE FOR GEMMA\n",
    "                    types.Part.from_text(\n",
    "                    text=\"\"\"Oferă un singur răspuns\"\"\"\n",
    "                    ),\n",
    "                ],\n",
    "            ),\n",
    "        )\n",
    "        # print(response)\n",
    "        if response.candidates[0].finish_reason.name != 'STOP':\n",
    "            raise Exception(\"Bad response\")\n",
    "        \n",
    "        return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate text with Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_client = GoogleAIClient(google_api_key, google_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapters = pd.read_csv(\"../dataframe_2800-5600ch_marker_chapters_FINAL_GEMINI.csv\")\n",
    "chapters = chapters.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46675\n",
      "Index(['original_index', 'document_id', 'sample', 'title'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(len(chapters))\n",
    "print(chapters.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_range= 21000\n",
    "end_range=26000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model:  gemini-2.5-flash-preview-05-20\n",
      "Generating from index:  0  to  20000\n",
      "Error processing row index 1662 (original_index 3324): 'NoneType' object is not subscriptable\n",
      "Processed 50 rows (up to index 8028), saving chunk to CSV...\n",
      "DataFrame successfully saved to gemini/FINAL_GEMINI_gap_0_20000_50_20250529_161701.csv\n",
      "Processed 100 rows (up to index 11997), saving chunk to CSV...\n",
      "DataFrame successfully saved to gemini/FINAL_GEMINI_gap_0_20000_100_20250529_161718.csv\n",
      "Error processing row index 14491 (original_index 28982): 'NoneType' object is not subscriptable\n",
      "Error processing row index 14976 (original_index 29952): 'NoneType' object is not subscriptable\n",
      "Error processing row index 15216 (original_index 30432): 'NoneType' object is not subscriptable\n",
      "Error processing row index 15217 (original_index 30434): 'NoneType' object is not subscriptable\n",
      "Error processing row index 15659 (original_index 31318): 'NoneType' object is not subscriptable\n",
      "Error processing row index 15662 (original_index 31324): 'NoneType' object is not subscriptable\n",
      "Processed 150 rows (up to index 19376), saving chunk to CSV...\n",
      "DataFrame successfully saved to gemini/FINAL_GEMINI_gap_0_20000_150_20250529_161737.csv\n",
      "Saving remaining 152 results...\n",
      "DataFrame successfully saved to gemini/FINAL_GEMINI_gap_0_20000_final_20250529_161740.csv\n",
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "print(\"Using model: \", api_client.model_name)\n",
    "print(\"Generating from index: \", start_range, \" to \", end_range)\n",
    "\n",
    "generated_results = []\n",
    "temperature_max_change = 0.2\n",
    "chunk_size = 50  # Define the number of rows to process before saving\n",
    "max_workers = 15 # Number of concurrent API calls\n",
    "queries = [\"rephrase\", \"summarize\", \"continue\"] #  [\"non_ai_doctorat\", \"non_ai_doctorat_summary\", \"non_ai_doctorat_continue\"] #\n",
    "\n",
    "def process_row(index):\n",
    "    \"\"\"Processes a single row to generate text using the API client.\"\"\"\n",
    "    row = chapters.iloc[index]\n",
    "    original_index = row[\"original_index\"]\n",
    "    chapter = row[\"sample\"]\n",
    "\n",
    "    temperature = 1 + np.random.uniform(-temperature_max_change, temperature_max_change)\n",
    "    query_id = index % len(queries) \n",
    "    query_task = queries[query_id]\n",
    "    \n",
    "    try:\n",
    "        generated = api_client.gen(chapter, query_task, temperature)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row index {index} (original_index {original_index}): {e}\")\n",
    "        generated = f\"ERROR: {e}\"\n",
    "\n",
    "    time.sleep(1) # Optional delay to avoid hitting API limits\n",
    "    return {\n",
    "        \"original_index\": original_index,\n",
    "        \"document_id\": row[\"document_id\"],\n",
    "        \"title\": row[\"title\"],\n",
    "        \"task\": query_task,\n",
    "        \"chapter\": chapter,\n",
    "        \"generated\": generated,\n",
    "        \"temperature\": temperature,\n",
    "    }\n",
    "\n",
    "# Use ThreadPoolExecutor for concurrent API calls\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    futures = {executor.submit(process_row, i): i for i in range(start_range, end_range)}\n",
    "    processed_count = 0\n",
    "\n",
    "    for future in as_completed(futures):\n",
    "        index = futures[future]\n",
    "        try:\n",
    "            result = future.result()\n",
    "            generated_results.append(result)\n",
    "            processed_count += 1\n",
    "\n",
    "            # Save in chunks\n",
    "            if processed_count % chunk_size == 0:\n",
    "                print(f\"Processed {processed_count} rows (up to index {index}), saving chunk to CSV...\")\n",
    "                generated_data_chunk = pd.DataFrame(generated_results)\n",
    "                to_csv(generated_data_chunk, f\"gemini/FINAL_GEMINI_gap_{start_range}_{end_range}_{processed_count}.csv\")\n",
    "\n",
    "\n",
    "        except Exception as exc:\n",
    "            print(f'Index {index} generated an exception: {exc}')\n",
    "\n",
    "# Save any remaining results after the loop finishes\n",
    "if generated_results:\n",
    "    print(f\"Saving remaining {len(generated_results)} results...\")\n",
    "    generated_data = pd.DataFrame(generated_results)\n",
    "    to_csv(generated_data, f\"gemini/FINAL_GEMINI_gap_{start_range}_{end_range}_final.csv\")\n",
    "                \n",
    "\n",
    "print(f\"Processing complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
